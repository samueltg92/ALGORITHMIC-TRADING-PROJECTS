{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-13 09:45:48.333368: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-05-13 09:45:48.334447: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-05-13 09:45:48.335355: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-05-13 09:45:48.488394: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-05-13 09:45:48.489521: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-05-13 09:45:48.490454: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-05-13 09:45:48.809967: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-05-13 09:45:48.811064: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-05-13 09:45:48.811861: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5122/5153 [============================>.] - ETA: 0s - loss: 8.4478e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-13 09:45:56.091376: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-05-13 09:45:56.092547: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-05-13 09:45:56.093463: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5153/5153 [==============================] - 9s 2ms/step - loss: 8.3982e-04 - val_loss: 2.0547e-05\n",
      "Epoch 2/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 3.4319e-06 - val_loss: 7.2992e-07\n",
      "Epoch 3/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 8.6114e-07 - val_loss: 2.0378e-07\n",
      "Epoch 4/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 8.8168e-07 - val_loss: 9.7976e-07\n",
      "Epoch 5/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 6.8296e-07 - val_loss: 8.6363e-07\n",
      "Epoch 6/100\n",
      "5153/5153 [==============================] - 8s 2ms/step - loss: 4.8013e-07 - val_loss: 8.6350e-08\n",
      "Epoch 7/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 5.2585e-07 - val_loss: 5.9509e-08\n",
      "Epoch 8/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 5.2063e-07 - val_loss: 9.5053e-09\n",
      "Epoch 9/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 3.5831e-07 - val_loss: 1.2244e-08\n",
      "Epoch 10/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 5.6845e-07 - val_loss: 8.2241e-09\n",
      "Epoch 11/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 4.6317e-07 - val_loss: 2.0444e-08\n",
      "Epoch 12/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 1.1053e-06 - val_loss: 1.0909e-08\n",
      "Epoch 13/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 3.9026e-07 - val_loss: 4.3143e-09\n",
      "Epoch 14/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 4.1141e-07 - val_loss: 9.9228e-09\n",
      "Epoch 15/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 3.9006e-07 - val_loss: 9.9894e-09\n",
      "Epoch 16/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 4.0546e-07 - val_loss: 8.5645e-09\n",
      "Epoch 17/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 3.6526e-07 - val_loss: 5.3555e-08\n",
      "Epoch 18/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 7.8447e-07 - val_loss: 5.2069e-09\n",
      "Epoch 19/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 2.4766e-07 - val_loss: 1.9264e-06\n",
      "Epoch 20/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 3.5166e-07 - val_loss: 5.9557e-09\n",
      "Epoch 21/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 3.3988e-07 - val_loss: 6.1346e-09\n",
      "Epoch 22/100\n",
      "5153/5153 [==============================] - 8s 2ms/step - loss: 3.5792e-07 - val_loss: 6.7124e-09\n",
      "Epoch 23/100\n",
      "5153/5153 [==============================] - 8s 2ms/step - loss: 3.9467e-07 - val_loss: 1.2453e-07\n",
      "Epoch 24/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 3.6049e-07 - val_loss: 1.0766e-07\n",
      "Epoch 25/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 3.1219e-07 - val_loss: 2.5411e-08\n",
      "Epoch 26/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 3.0033e-07 - val_loss: 3.0611e-07\n",
      "Epoch 27/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 4.5824e-07 - val_loss: 1.5293e-07\n",
      "Epoch 28/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 3.0540e-07 - val_loss: 3.0188e-07\n",
      "Epoch 29/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 5.2903e-07 - val_loss: 2.8007e-08\n",
      "Epoch 30/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 3.6489e-07 - val_loss: 1.0779e-08\n",
      "Epoch 31/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 3.0520e-07 - val_loss: 2.2959e-07\n",
      "Epoch 32/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 2.9304e-07 - val_loss: 3.7710e-08\n",
      "Epoch 33/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 4.1404e-07 - val_loss: 5.4373e-09\n",
      "Epoch 34/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 4.0251e-07 - val_loss: 1.5773e-08\n",
      "Epoch 35/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 2.5486e-07 - val_loss: 2.1470e-07\n",
      "Epoch 36/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 2.9223e-07 - val_loss: 9.6060e-09\n",
      "Epoch 37/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 3.4411e-07 - val_loss: 5.3242e-09\n",
      "Epoch 38/100\n",
      "5153/5153 [==============================] - 8s 2ms/step - loss: 2.3188e-07 - val_loss: 6.6384e-09\n",
      "Epoch 39/100\n",
      "5153/5153 [==============================] - 8s 2ms/step - loss: 2.6805e-07 - val_loss: 4.8343e-09\n",
      "Epoch 40/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 2.5760e-07 - val_loss: 1.5754e-08\n",
      "Epoch 41/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 2.4780e-07 - val_loss: 4.5203e-07\n",
      "Epoch 42/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 3.0736e-07 - val_loss: 5.5629e-06\n",
      "Epoch 43/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 3.2990e-07 - val_loss: 1.1880e-08\n",
      "Epoch 44/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 2.8836e-07 - val_loss: 1.8166e-08\n",
      "Epoch 45/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 3.4525e-07 - val_loss: 4.7134e-09\n",
      "Epoch 46/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 2.3575e-07 - val_loss: 2.5041e-08\n",
      "Epoch 47/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 2.5701e-07 - val_loss: 6.2604e-09\n",
      "Epoch 48/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 3.2372e-07 - val_loss: 3.9663e-07\n",
      "Epoch 49/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 3.0421e-07 - val_loss: 2.0878e-07\n",
      "Epoch 50/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 4.0874e-07 - val_loss: 1.6548e-05\n",
      "Epoch 51/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 2.6616e-07 - val_loss: 1.6625e-07\n",
      "Epoch 52/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 2.8370e-07 - val_loss: 1.3596e-08\n",
      "Epoch 53/100\n",
      "5153/5153 [==============================] - 7s 1ms/step - loss: 3.0118e-07 - val_loss: 3.7049e-07\n",
      "Epoch 54/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 2.2537e-07 - val_loss: 3.0978e-07\n",
      "Epoch 55/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 2.8325e-07 - val_loss: 4.6412e-08\n",
      "Epoch 56/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 2.6923e-07 - val_loss: 1.7503e-07\n",
      "Epoch 57/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 2.2353e-07 - val_loss: 1.4329e-08\n",
      "Epoch 58/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 3.8004e-07 - val_loss: 7.4853e-08\n",
      "Epoch 59/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 2.1387e-07 - val_loss: 4.7249e-08\n",
      "Epoch 60/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 2.9464e-07 - val_loss: 3.2503e-08\n",
      "Epoch 61/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 3.1905e-07 - val_loss: 3.0793e-08\n",
      "Epoch 62/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 3.3324e-07 - val_loss: 9.6032e-09\n",
      "Epoch 63/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 2.8720e-07 - val_loss: 2.3250e-07\n",
      "Epoch 64/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 2.1208e-07 - val_loss: 2.0143e-08\n",
      "Epoch 65/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 2.7144e-07 - val_loss: 2.2331e-08\n",
      "Epoch 66/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 2.5845e-07 - val_loss: 3.1563e-08\n",
      "Epoch 67/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 2.6599e-07 - val_loss: 5.1386e-09\n",
      "Epoch 68/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 2.9771e-07 - val_loss: 7.9932e-09\n",
      "Epoch 69/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 2.8822e-07 - val_loss: 1.7548e-08\n",
      "Epoch 70/100\n",
      "5153/5153 [==============================] - 7s 1ms/step - loss: 2.4619e-07 - val_loss: 1.4969e-08\n",
      "Epoch 71/100\n",
      "5153/5153 [==============================] - 8s 2ms/step - loss: 3.1034e-07 - val_loss: 4.2062e-08\n",
      "Epoch 72/100\n",
      "5153/5153 [==============================] - 7s 1ms/step - loss: 2.0835e-07 - val_loss: 5.7900e-07\n",
      "Epoch 73/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 2.0047e-07 - val_loss: 4.7350e-08\n",
      "Epoch 74/100\n",
      "5153/5153 [==============================] - 7s 1ms/step - loss: 2.9740e-07 - val_loss: 1.1575e-08\n",
      "Epoch 75/100\n",
      "5153/5153 [==============================] - 7s 1ms/step - loss: 2.7706e-07 - val_loss: 6.7545e-09\n",
      "Epoch 76/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 1.8821e-07 - val_loss: 7.2399e-08\n",
      "Epoch 77/100\n",
      "5153/5153 [==============================] - 8s 2ms/step - loss: 2.6388e-07 - val_loss: 5.1134e-08\n",
      "Epoch 78/100\n",
      "5153/5153 [==============================] - 7s 1ms/step - loss: 3.7789e-07 - val_loss: 1.5509e-06\n",
      "Epoch 79/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 2.6860e-07 - val_loss: 7.2645e-09\n",
      "Epoch 80/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 1.7113e-07 - val_loss: 1.3028e-08\n",
      "Epoch 81/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 2.2829e-07 - val_loss: 5.0703e-08\n",
      "Epoch 82/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 2.3879e-07 - val_loss: 7.3150e-08\n",
      "Epoch 83/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 2.2154e-07 - val_loss: 1.7463e-08\n",
      "Epoch 84/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 2.1325e-07 - val_loss: 4.5878e-07\n",
      "Epoch 85/100\n",
      "5153/5153 [==============================] - 8s 2ms/step - loss: 2.2101e-07 - val_loss: 5.3000e-08\n",
      "Epoch 86/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 2.0079e-07 - val_loss: 5.4112e-08\n",
      "Epoch 87/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 2.7261e-07 - val_loss: 1.2552e-08\n",
      "Epoch 88/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 1.7384e-07 - val_loss: 1.6541e-06\n",
      "Epoch 89/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 2.8104e-07 - val_loss: 2.0697e-08\n",
      "Epoch 90/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 2.1598e-07 - val_loss: 2.5624e-08\n",
      "Epoch 91/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 2.1260e-07 - val_loss: 6.8805e-08\n",
      "Epoch 92/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 2.3207e-07 - val_loss: 6.5082e-08\n",
      "Epoch 93/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 1.8254e-07 - val_loss: 4.9195e-09\n",
      "Epoch 94/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 1.7716e-07 - val_loss: 1.2629e-07\n",
      "Epoch 95/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 2.0815e-07 - val_loss: 5.3852e-08\n",
      "Epoch 96/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 1.8325e-07 - val_loss: 9.9808e-08\n",
      "Epoch 97/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 2.7749e-07 - val_loss: 1.1041e-07\n",
      "Epoch 98/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 1.7727e-07 - val_loss: 2.4720e-07\n",
      "Epoch 99/100\n",
      "5153/5153 [==============================] - 8s 2ms/step - loss: 1.7322e-07 - val_loss: 2.6861e-08\n",
      "Epoch 100/100\n",
      "5153/5153 [==============================] - 8s 1ms/step - loss: 2.8941e-07 - val_loss: 1.1892e-07\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Este código carga los datos históricos de precios de Bitcoin desde el cliente de Binance y los procesa \n",
    "para la entrada del modelo. Luego, divide los datos en conjuntos de entrenamiento y validación, \n",
    "define un modelo de red neuronal LSTM y lo entrena durante 100 épocas utilizando el optimizador \n",
    "Adam y la función de pérdida de error cuadrático medio (mse). Finalmente, guarda el modelo \n",
    "entrenado en un archivo llamado \"btc_price_prediction_model.h5\"\n",
    "'''\n",
    "\n",
    "'''\n",
    "TO-DO LIST:\n",
    "1. Implementar al entrenamiento en el corto-mediano plazo: \n",
    "    * la lectura de opiniones en RRSS, \n",
    "    * análisis de traders externos \n",
    "    * adición de indicadores técnicos\n",
    "2. Cómo aumentar las redes neuronales?\n",
    "3. A la hora de graficar los precios de predicción, hay que hacerlo a modo de área predecida y no de precio en especifico \n",
    "'''\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from binance.client import Client\n",
    "\n",
    "# Autenticación en la API de Binance\n",
    "api_key = 'T4Bw573BSJCbaHscSo8jn37lE1SOoGsNircF8f7B061WIKBxNkP5nx68vvAev9uk'\n",
    "api_secret = 'p7VN6HHKR6ZtGkfEeZC3FongJPkR2yr7AjPtPDC8IeaOkASdKZYCKyTuiIfGw57q'\n",
    "client = Client(api_key, api_secret)\n",
    "\n",
    "# Obtención de los datos históricos de precios de Bitcoin\n",
    "klines = client.futures_historical_klines(\"BTCUSDT\", Client.KLINE_INTERVAL_15MINUTE, \"10 year ago UTC\") #intervalo de 15minutes a 10 años\n",
    "\n",
    "# Creación de un dataframe con los datos\n",
    "data = pd.DataFrame(klines, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume', 'close_time', \n",
    "                                     'quote_asset_volume', 'number_of_trades', 'taker_buy_base_asset_volume', \n",
    "                                     'taker_buy_quote_asset_volume', 'ignore'])\n",
    "\n",
    "# Eliminación de las columnas que no se van a utilizar\n",
    "data = data.drop(['timestamp', 'high', 'low', 'volume', 'close_time', 'quote_asset_volume', 'number_of_trades', \n",
    "                  'taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume', 'ignore'], axis=1)\n",
    "\n",
    "# Conversión de los precios a float\n",
    "data['open'] = data['open'].astype(float)\n",
    "data['close'] = data['close'].astype(float)\n",
    "\n",
    "# Normalizar los datos\n",
    "max_value = data['close'].max()\n",
    "min_value = data['close'].min()\n",
    "data['close'] = (data['close'] - min_value) / (max_value - min_value)\n",
    "\n",
    "# Procesar los datos para la entrada del modelo\n",
    "data['t-96'] = data['close'].shift(96)\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "X = np.array(data[['close', 't-96']])\n",
    "X = np.reshape(X, (X.shape[0], X.shape[1], 1))\n",
    "y = np.array(data['close'])\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y validación\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Definir el modelo de red neuronal\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.LSTM(50, input_shape=(X.shape[1], 1)),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "# Entrenar el modelo\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=20, validation_data=(X_val, y_val))\n",
    "\n",
    "# Guardar el modelo entrenado\n",
    "model.save('modelos de entrenamiento/btc_price_prediction_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-La función train_test_split de Sklearn es utilizada para dividir el conjunto de datos en dos subconjuntos: uno para entrenamiento y otro para prueba. La función toma cuatro argumentos: los datos de entrada (data.iloc[:, :-1]), las etiquetas (data.iloc[:, -1]), el tamaño del conjunto de prueba (test_size=0.2) y la semilla aleatoria (random_state=42) para garantizar la reproducibilidad del experimento. Esta función devuelve cuatro variables: X_train, X_test, y_train y y_test. X_train y y_train son los subconjuntos de entrenamiento, mientras que X_test y y_test son los subconjuntos de prueba.\n",
    "\n",
    "-tf.keras.Sequential es una función que se utiliza para construir un modelo secuencial, es decir, una pila lineal de capas de redes neuronales. En este caso, se está construyendo un modelo con dos capas densas (Dense). La primera capa tiene 10 unidades, toma una entrada de 4 características (input_shape=[4]) y utiliza la función de activación ReLU (activation='relu'). La segunda capa tiene 3 unidades y utiliza la función de activación Softmax (activation='softmax').\n",
    "\n",
    "-model.compile se utiliza para compilar el modelo creado. Toma tres argumentos: el optimizador (optimizer=tf.keras.optimizers.Adam(0.01)), la función de pérdida (loss='sparse_categorical_crossentropy') y las métricas (metrics=['accuracy']). En este caso, se utiliza el optimizador Adam con una tasa de aprendizaje de 0.01, la función de pérdida sparse_categorical_crossentropy y la métrica de precisión (accuracy).\n",
    "\n",
    "-Después de compilar el modelo, se procede a entrenarlo utilizando la función model.fit(). Esta función recibe varios parámetros importantes:\n",
    "\n",
    "x_train y y_train: son los datos de entrenamiento que se utilizarán para ajustar el modelo.\n",
    "\n",
    "validation_data: es un conjunto de datos que se utilizará para validar el modelo durante el entrenamiento. En este caso, se utiliza el 20% de los datos de entrenamiento como conjunto de validación.\n",
    "\n",
    "epochs: es la cantidad de veces que el modelo verá todos los datos de entrenamiento durante el entrenamiento. En este caso, se utilizan 50 épocas.\n",
    "\n",
    "batch_size: es el número de muestras que se utilizarán para calcular el error y actualizar los pesos del modelo. En este caso, se utilizan 32 muestras por lote.\n",
    "\n",
    "verbose: es un valor que indica la cantidad de información que se mostrará durante el entrenamiento. En este caso, se utiliza verbose=2, lo que significa que se mostrará una barra de progreso durante el entrenamiento.\n",
    "\n",
    "callbacks: es una lista de objetos que se utilizarán durante el entrenamiento para realizar acciones específicas en ciertos momentos. En este caso, se utiliza el objeto EarlyStopping para detener el entrenamiento si el modelo deja de mejorar en el conjunto de validación.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TRADING",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
