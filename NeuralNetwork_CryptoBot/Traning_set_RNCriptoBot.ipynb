{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-01 16:34:29.371084: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-01 16:34:29.426605: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-01 16:34:29.427382: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-01 16:34:30.327185: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-09-01 16:37:41.194805: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-09-01 16:37:41.195419: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-01 16:37:41.563612: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-09-01 16:37:41.567454: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-09-01 16:37:41.569901: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-09-01 16:37:42.029016: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-09-01 16:37:42.032672: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-09-01 16:37:42.035395: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-09-01 16:37:42.721556: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-09-01 16:37:42.724874: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-09-01 16:37:42.727461: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5579/5581 [============================>.] - ETA: 0s - loss: 0.0010"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-01 16:38:20.023426: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-09-01 16:38:20.026187: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-09-01 16:38:20.028777: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5581/5581 [==============================] - 41s 7ms/step - loss: 0.0010 - val_loss: 9.4399e-06\n",
      "Epoch 2/100\n",
      "5581/5581 [==============================] - 32s 6ms/step - loss: 2.4637e-06 - val_loss: 2.2707e-07\n",
      "Epoch 3/100\n",
      "5581/5581 [==============================] - 31s 6ms/step - loss: 7.2556e-07 - val_loss: 1.7052e-07\n",
      "Epoch 4/100\n",
      "5581/5581 [==============================] - 29s 5ms/step - loss: 5.7879e-07 - val_loss: 1.1205e-06\n",
      "Epoch 5/100\n",
      "5581/5581 [==============================] - 30s 5ms/step - loss: 5.2100e-07 - val_loss: 1.0781e-07\n",
      "Epoch 6/100\n",
      "5581/5581 [==============================] - 28s 5ms/step - loss: 4.8140e-07 - val_loss: 9.1384e-07\n",
      "Epoch 7/100\n",
      "5581/5581 [==============================] - 31s 5ms/step - loss: 3.8647e-07 - val_loss: 1.1316e-07\n",
      "Epoch 8/100\n",
      "5581/5581 [==============================] - 32s 6ms/step - loss: 5.4512e-07 - val_loss: 1.7145e-08\n",
      "Epoch 9/100\n",
      "5581/5581 [==============================] - 33s 6ms/step - loss: 4.1725e-07 - val_loss: 1.8617e-08\n",
      "Epoch 10/100\n",
      "5581/5581 [==============================] - 33s 6ms/step - loss: 4.0806e-07 - val_loss: 5.1697e-05\n",
      "Epoch 11/100\n",
      "5581/5581 [==============================] - 29s 5ms/step - loss: 4.9383e-07 - val_loss: 6.8668e-09\n",
      "Epoch 12/100\n",
      "5581/5581 [==============================] - 33s 6ms/step - loss: 3.3974e-07 - val_loss: 3.2248e-08\n",
      "Epoch 13/100\n",
      "5581/5581 [==============================] - 34s 6ms/step - loss: 4.7805e-07 - val_loss: 1.8863e-08\n",
      "Epoch 14/100\n",
      "5581/5581 [==============================] - 33s 6ms/step - loss: 3.9002e-07 - val_loss: 6.0717e-08\n",
      "Epoch 15/100\n",
      "5581/5581 [==============================] - 37s 7ms/step - loss: 2.7675e-07 - val_loss: 1.8935e-07\n",
      "Epoch 16/100\n",
      "5581/5581 [==============================] - 32s 6ms/step - loss: 4.4834e-07 - val_loss: 5.8697e-08\n",
      "Epoch 17/100\n",
      "5581/5581 [==============================] - 35s 6ms/step - loss: 3.5090e-07 - val_loss: 2.6351e-06\n",
      "Epoch 18/100\n",
      "5581/5581 [==============================] - 31s 6ms/step - loss: 3.7919e-07 - val_loss: 4.1249e-07\n",
      "Epoch 19/100\n",
      "5581/5581 [==============================] - 28s 5ms/step - loss: 3.0204e-07 - val_loss: 5.1572e-08\n",
      "Epoch 20/100\n",
      "5581/5581 [==============================] - 33s 6ms/step - loss: 3.3597e-07 - val_loss: 7.2495e-08\n",
      "Epoch 21/100\n",
      "5581/5581 [==============================] - 29s 5ms/step - loss: 3.8973e-07 - val_loss: 6.3044e-09\n",
      "Epoch 22/100\n",
      "5581/5581 [==============================] - 28s 5ms/step - loss: 3.2126e-07 - val_loss: 2.8036e-08\n",
      "Epoch 23/100\n",
      "5581/5581 [==============================] - 27s 5ms/step - loss: 3.1560e-07 - val_loss: 2.4627e-07\n",
      "Epoch 24/100\n",
      "5581/5581 [==============================] - 27s 5ms/step - loss: 3.3900e-07 - val_loss: 5.1412e-08\n",
      "Epoch 25/100\n",
      "5581/5581 [==============================] - 26s 5ms/step - loss: 3.1118e-07 - val_loss: 8.2048e-07\n",
      "Epoch 26/100\n",
      "5581/5581 [==============================] - 26s 5ms/step - loss: 3.0617e-07 - val_loss: 2.7372e-08\n",
      "Epoch 27/100\n",
      "5581/5581 [==============================] - 26s 5ms/step - loss: 2.7032e-07 - val_loss: 4.1440e-07\n",
      "Epoch 28/100\n",
      "5581/5581 [==============================] - 27s 5ms/step - loss: 3.0175e-07 - val_loss: 2.4615e-07\n",
      "Epoch 29/100\n",
      "5581/5581 [==============================] - 28s 5ms/step - loss: 2.7797e-07 - val_loss: 3.1978e-08\n",
      "Epoch 30/100\n",
      "5581/5581 [==============================] - 31s 6ms/step - loss: 2.8665e-07 - val_loss: 3.4773e-07\n",
      "Epoch 31/100\n",
      "5581/5581 [==============================] - 27s 5ms/step - loss: 4.6438e-07 - val_loss: 4.9193e-07\n",
      "Epoch 32/100\n",
      "5581/5581 [==============================] - 26s 5ms/step - loss: 2.8345e-07 - val_loss: 1.4040e-08\n",
      "Epoch 33/100\n",
      "5581/5581 [==============================] - 26s 5ms/step - loss: 3.2447e-07 - val_loss: 2.2965e-07\n",
      "Epoch 34/100\n",
      "5581/5581 [==============================] - 28s 5ms/step - loss: 2.8051e-07 - val_loss: 5.8192e-08\n",
      "Epoch 35/100\n",
      "5581/5581 [==============================] - 30s 5ms/step - loss: 2.3975e-07 - val_loss: 9.4845e-07\n",
      "Epoch 36/100\n",
      "5581/5581 [==============================] - 27s 5ms/step - loss: 2.7991e-07 - val_loss: 4.0295e-08\n",
      "Epoch 37/100\n",
      "5581/5581 [==============================] - 27s 5ms/step - loss: 2.7028e-07 - val_loss: 2.8465e-08\n",
      "Epoch 38/100\n",
      "5581/5581 [==============================] - 27s 5ms/step - loss: 2.6237e-07 - val_loss: 3.8208e-08\n",
      "Epoch 39/100\n",
      "5581/5581 [==============================] - 27s 5ms/step - loss: 3.2469e-07 - val_loss: 2.3243e-06\n",
      "Epoch 40/100\n",
      "5581/5581 [==============================] - 26s 5ms/step - loss: 3.2691e-07 - val_loss: 1.0150e-07\n",
      "Epoch 41/100\n",
      "5581/5581 [==============================] - 26s 5ms/step - loss: 2.7741e-07 - val_loss: 6.1114e-09\n",
      "Epoch 42/100\n",
      "5581/5581 [==============================] - 26s 5ms/step - loss: 2.9418e-07 - val_loss: 9.3973e-08\n",
      "Epoch 43/100\n",
      "5581/5581 [==============================] - 26s 5ms/step - loss: 4.0481e-07 - val_loss: 1.4032e-08\n",
      "Epoch 44/100\n",
      "5581/5581 [==============================] - 26s 5ms/step - loss: 3.8143e-07 - val_loss: 4.0817e-08\n",
      "Epoch 45/100\n",
      "5581/5581 [==============================] - 26s 5ms/step - loss: 2.6003e-07 - val_loss: 4.4221e-08\n",
      "Epoch 46/100\n",
      "5581/5581 [==============================] - 26s 5ms/step - loss: 2.6318e-07 - val_loss: 1.4333e-07\n",
      "Epoch 47/100\n",
      "5581/5581 [==============================] - 27s 5ms/step - loss: 2.3976e-07 - val_loss: 1.2368e-08\n",
      "Epoch 48/100\n",
      "5581/5581 [==============================] - 27s 5ms/step - loss: 2.3987e-07 - val_loss: 6.5333e-08\n",
      "Epoch 49/100\n",
      "5581/5581 [==============================] - 26s 5ms/step - loss: 2.2673e-07 - val_loss: 8.7144e-09\n",
      "Epoch 50/100\n",
      "5581/5581 [==============================] - 26s 5ms/step - loss: 2.4274e-07 - val_loss: 8.1462e-07\n",
      "Epoch 51/100\n",
      "5581/5581 [==============================] - 27s 5ms/step - loss: 2.8354e-07 - val_loss: 8.6942e-09\n",
      "Epoch 52/100\n",
      "5581/5581 [==============================] - 27s 5ms/step - loss: 2.3525e-07 - val_loss: 1.0792e-07\n",
      "Epoch 53/100\n",
      "5581/5581 [==============================] - 26s 5ms/step - loss: 2.0869e-07 - val_loss: 3.1654e-08\n",
      "Epoch 54/100\n",
      "5581/5581 [==============================] - 26s 5ms/step - loss: 2.8982e-07 - val_loss: 2.2192e-07\n",
      "Epoch 55/100\n",
      "5581/5581 [==============================] - 26s 5ms/step - loss: 2.5808e-07 - val_loss: 1.5832e-08\n",
      "Epoch 56/100\n",
      "5581/5581 [==============================] - 27s 5ms/step - loss: 2.7746e-07 - val_loss: 7.6994e-09\n",
      "Epoch 57/100\n",
      "5581/5581 [==============================] - 26s 5ms/step - loss: 1.6775e-07 - val_loss: 8.2974e-08\n",
      "Epoch 58/100\n",
      "5581/5581 [==============================] - 26s 5ms/step - loss: 2.1228e-07 - val_loss: 1.7845e-06\n",
      "Epoch 59/100\n",
      "5581/5581 [==============================] - 26s 5ms/step - loss: 2.4650e-07 - val_loss: 1.2622e-08\n",
      "Epoch 60/100\n",
      "5581/5581 [==============================] - 26s 5ms/step - loss: 2.9174e-07 - val_loss: 1.5576e-08\n",
      "Epoch 61/100\n",
      "5581/5581 [==============================] - 30s 5ms/step - loss: 1.9511e-07 - val_loss: 2.4098e-08\n",
      "Epoch 62/100\n",
      "5581/5581 [==============================] - 26s 5ms/step - loss: 2.1715e-07 - val_loss: 3.3250e-07\n",
      "Epoch 63/100\n",
      "5581/5581 [==============================] - 26s 5ms/step - loss: 2.0974e-07 - val_loss: 2.7829e-07\n",
      "Epoch 64/100\n",
      "5581/5581 [==============================] - 26s 5ms/step - loss: 1.9023e-07 - val_loss: 6.7440e-08\n",
      "Epoch 65/100\n",
      "5581/5581 [==============================] - 27s 5ms/step - loss: 2.7304e-07 - val_loss: 7.3166e-09\n",
      "Epoch 66/100\n",
      "5581/5581 [==============================] - 26s 5ms/step - loss: 2.0267e-07 - val_loss: 3.2651e-07\n",
      "Epoch 67/100\n",
      "5581/5581 [==============================] - 26s 5ms/step - loss: 2.3823e-07 - val_loss: 7.2298e-07\n",
      "Epoch 68/100\n",
      "5581/5581 [==============================] - 26s 5ms/step - loss: 2.1301e-07 - val_loss: 9.6777e-08\n",
      "Epoch 69/100\n",
      "5581/5581 [==============================] - 26s 5ms/step - loss: 2.2669e-07 - val_loss: 7.5468e-09\n",
      "Epoch 70/100\n",
      "5581/5581 [==============================] - 26s 5ms/step - loss: 1.8991e-07 - val_loss: 9.3883e-08\n",
      "Epoch 71/100\n",
      "5581/5581 [==============================] - 26s 5ms/step - loss: 3.0462e-07 - val_loss: 1.4284e-08\n",
      "Epoch 72/100\n",
      "5581/5581 [==============================] - 26s 5ms/step - loss: 2.2221e-07 - val_loss: 3.7114e-07\n",
      "Epoch 73/100\n",
      "5581/5581 [==============================] - 26s 5ms/step - loss: 2.1127e-07 - val_loss: 4.4156e-08\n",
      "Epoch 74/100\n",
      "5581/5581 [==============================] - 26s 5ms/step - loss: 2.2120e-07 - val_loss: 5.3306e-08\n",
      "Epoch 75/100\n",
      "5581/5581 [==============================] - 26s 5ms/step - loss: 1.8831e-07 - val_loss: 1.0716e-07\n",
      "Epoch 76/100\n",
      "5581/5581 [==============================] - 26s 5ms/step - loss: 2.0577e-07 - val_loss: 7.9340e-08\n",
      "Epoch 77/100\n",
      "5581/5581 [==============================] - 26s 5ms/step - loss: 2.3187e-07 - val_loss: 1.4542e-08\n",
      "Epoch 78/100\n",
      "5581/5581 [==============================] - 27s 5ms/step - loss: 1.9665e-07 - val_loss: 7.2995e-08\n",
      "Epoch 79/100\n",
      "5581/5581 [==============================] - 26s 5ms/step - loss: 1.6485e-07 - val_loss: 2.9149e-08\n",
      "Epoch 80/100\n",
      "5581/5581 [==============================] - 26s 5ms/step - loss: 1.8077e-07 - val_loss: 3.9576e-07\n",
      "Epoch 81/100\n",
      "5581/5581 [==============================] - 27s 5ms/step - loss: 2.0120e-07 - val_loss: 1.2556e-08\n",
      "Epoch 82/100\n",
      "5581/5581 [==============================] - 26s 5ms/step - loss: 2.0125e-07 - val_loss: 1.4905e-07\n",
      "Epoch 83/100\n",
      "5581/5581 [==============================] - 26s 5ms/step - loss: 2.0797e-07 - val_loss: 2.3034e-08\n",
      "Epoch 84/100\n",
      "5581/5581 [==============================] - 28s 5ms/step - loss: 1.8067e-07 - val_loss: 4.0380e-08\n",
      "Epoch 85/100\n",
      "5581/5581 [==============================] - 26s 5ms/step - loss: 2.1079e-07 - val_loss: 1.7438e-07\n",
      "Epoch 86/100\n",
      "5581/5581 [==============================] - 26s 5ms/step - loss: 1.5093e-07 - val_loss: 3.8935e-08\n",
      "Epoch 87/100\n",
      "5581/5581 [==============================] - 26s 5ms/step - loss: 2.5905e-07 - val_loss: 1.1503e-08\n",
      "Epoch 88/100\n",
      "5581/5581 [==============================] - 26s 5ms/step - loss: 1.5161e-07 - val_loss: 7.2146e-09\n",
      "Epoch 89/100\n",
      "5581/5581 [==============================] - 26s 5ms/step - loss: 2.1745e-07 - val_loss: 9.7455e-07\n",
      "Epoch 90/100\n",
      "5581/5581 [==============================] - 26s 5ms/step - loss: 1.7151e-07 - val_loss: 1.4732e-08\n",
      "Epoch 91/100\n",
      "5581/5581 [==============================] - 26s 5ms/step - loss: 2.2461e-07 - val_loss: 1.3114e-07\n",
      "Epoch 92/100\n",
      "5581/5581 [==============================] - 26s 5ms/step - loss: 2.0488e-07 - val_loss: 3.4320e-07\n",
      "Epoch 93/100\n",
      "5581/5581 [==============================] - 26s 5ms/step - loss: 1.5579e-07 - val_loss: 3.2766e-07\n",
      "Epoch 94/100\n",
      "5581/5581 [==============================] - 26s 5ms/step - loss: 1.6058e-07 - val_loss: 4.9622e-09\n",
      "Epoch 95/100\n",
      "5581/5581 [==============================] - 26s 5ms/step - loss: 1.8902e-07 - val_loss: 1.2652e-08\n",
      "Epoch 96/100\n",
      "5581/5581 [==============================] - 26s 5ms/step - loss: 1.6146e-07 - val_loss: 3.0636e-08\n",
      "Epoch 97/100\n",
      "5581/5581 [==============================] - 26s 5ms/step - loss: 1.7926e-07 - val_loss: 1.3012e-08\n",
      "Epoch 98/100\n",
      "5581/5581 [==============================] - 29s 5ms/step - loss: 1.4442e-07 - val_loss: 3.6686e-07\n",
      "Epoch 99/100\n",
      "5581/5581 [==============================] - 26s 5ms/step - loss: 1.8394e-07 - val_loss: 1.5435e-07\n",
      "Epoch 100/100\n",
      "5581/5581 [==============================] - 27s 5ms/step - loss: 1.5522e-07 - val_loss: 4.3571e-09\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Este código carga los datos históricos de precios de Bitcoin desde el cliente de Binance y los procesa \n",
    "para la entrada del modelo. Luego, divide los datos en conjuntos de entrenamiento y validación, \n",
    "define un modelo de red neuronal LSTM y lo entrena durante 100 épocas utilizando el optimizador \n",
    "Adam y la función de pérdida de error cuadrático medio (mse). Finalmente, guarda el modelo \n",
    "entrenado en un archivo llamado \"btc_price_prediction_model.h5\"\n",
    "'''\n",
    "\n",
    "'''\n",
    "TO-DO LIST:\n",
    "1. Implementar al entrenamiento en el corto-mediano plazo: \n",
    "    * la lectura de opiniones en RRSS, \n",
    "    * análisis de traders externos \n",
    "    * adición de indicadores técnicos\n",
    "2. Cómo aumentar las redes neuronales?\n",
    "3. A la hora de graficar los precios de predicción, hay que hacerlo a modo de área predecida y no de precio en especifico \n",
    "'''\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from binance.client import Client\n",
    "\n",
    "# Autenticación en la API de Binance\n",
    "api_key = 'T4Bw573BSJCbaHscSo8jn37lE1SOoGsNircF8f7B061WIKBxNkP5nx68vvAev9uk'\n",
    "api_secret = 'p7VN6HHKR6ZtGkfEeZC3FongJPkR2yr7AjPtPDC8IeaOkASdKZYCKyTuiIfGw57q'\n",
    "client = Client(api_key, api_secret)\n",
    "\n",
    "# Obtención de los datos históricos de precios de Bitcoin\n",
    "klines = client.futures_historical_klines(\"BTCUSDT\", Client.KLINE_INTERVAL_15MINUTE, \"10 year ago UTC\") #intervalo de 15minutes a 10 años\n",
    "\n",
    "# Creación de un dataframe con los datos\n",
    "data = pd.DataFrame(klines, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume', 'close_time', \n",
    "                                     'quote_asset_volume', 'number_of_trades', 'taker_buy_base_asset_volume', \n",
    "                                     'taker_buy_quote_asset_volume', 'ignore'])\n",
    "\n",
    "# Eliminación de las columnas que no se van a utilizar\n",
    "data = data.drop(['timestamp', 'high', 'low', 'volume', 'close_time', 'quote_asset_volume', 'number_of_trades', \n",
    "                  'taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume', 'ignore'], axis=1)\n",
    "\n",
    "# Conversión de los precios a float\n",
    "data['open'] = data['open'].astype(float)\n",
    "data['close'] = data['close'].astype(float)\n",
    "\n",
    "# Normalizar los datos\n",
    "max_value = data['close'].max()\n",
    "min_value = data['close'].min()\n",
    "data['close'] = (data['close'] - min_value) / (max_value - min_value)\n",
    "\n",
    "# Procesar los datos para la entrada del modelo\n",
    "data['t-96'] = data['close'].shift(96)\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "X = np.array(data[['close', 't-96']])\n",
    "X = np.reshape(X, (X.shape[0], X.shape[1], 1))\n",
    "y = np.array(data['close'])\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y validación\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Definir el modelo de red neuronal\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.LSTM(50, input_shape=(X.shape[1], 1)),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "# Entrenar el modelo\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=20, validation_data=(X_val, y_val))\n",
    "\n",
    "# Guardar el modelo entrenado\n",
    "model.save('modelos de entrenamiento/btc_price_prediction_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-La función train_test_split de Sklearn es utilizada para dividir el conjunto de datos en dos subconjuntos: uno para entrenamiento y otro para prueba. La función toma cuatro argumentos: los datos de entrada (data.iloc[:, :-1]), las etiquetas (data.iloc[:, -1]), el tamaño del conjunto de prueba (test_size=0.2) y la semilla aleatoria (random_state=42) para garantizar la reproducibilidad del experimento. Esta función devuelve cuatro variables: X_train, X_test, y_train y y_test. X_train y y_train son los subconjuntos de entrenamiento, mientras que X_test y y_test son los subconjuntos de prueba.\n",
    "\n",
    "-tf.keras.Sequential es una función que se utiliza para construir un modelo secuencial, es decir, una pila lineal de capas de redes neuronales. En este caso, se está construyendo un modelo con dos capas densas (Dense). La primera capa tiene 10 unidades, toma una entrada de 4 características (input_shape=[4]) y utiliza la función de activación ReLU (activation='relu'). La segunda capa tiene 3 unidades y utiliza la función de activación Softmax (activation='softmax').\n",
    "\n",
    "-model.compile se utiliza para compilar el modelo creado. Toma tres argumentos: el optimizador (optimizer=tf.keras.optimizers.Adam(0.01)), la función de pérdida (loss='sparse_categorical_crossentropy') y las métricas (metrics=['accuracy']). En este caso, se utiliza el optimizador Adam con una tasa de aprendizaje de 0.01, la función de pérdida sparse_categorical_crossentropy y la métrica de precisión (accuracy).\n",
    "\n",
    "-Después de compilar el modelo, se procede a entrenarlo utilizando la función model.fit(). Esta función recibe varios parámetros importantes:\n",
    "\n",
    "x_train y y_train: son los datos de entrenamiento que se utilizarán para ajustar el modelo.\n",
    "\n",
    "validation_data: es un conjunto de datos que se utilizará para validar el modelo durante el entrenamiento. En este caso, se utiliza el 20% de los datos de entrenamiento como conjunto de validación.\n",
    "\n",
    "epochs: es la cantidad de veces que el modelo verá todos los datos de entrenamiento durante el entrenamiento. En este caso, se utilizan 50 épocas.\n",
    "\n",
    "batch_size: es el número de muestras que se utilizarán para calcular el error y actualizar los pesos del modelo. En este caso, se utilizan 32 muestras por lote.\n",
    "\n",
    "verbose: es un valor que indica la cantidad de información que se mostrará durante el entrenamiento. En este caso, se utiliza verbose=2, lo que significa que se mostrará una barra de progreso durante el entrenamiento.\n",
    "\n",
    "callbacks: es una lista de objetos que se utilizarán durante el entrenamiento para realizar acciones específicas en ciertos momentos. En este caso, se utiliza el objeto EarlyStopping para detener el entrenamiento si el modelo deja de mejorar en el conjunto de validación.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TRADING",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
