{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-14 14:21:00.321903: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-14 14:21:00.322209: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-05-14 14:21:00.458619: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-05-14 14:21:00.459426: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-05-14 14:21:00.460308: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-14 14:21:00.566789: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-05-14 14:21:00.567844: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-05-14 14:21:00.568787: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-05-14 14:21:00.767719: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-05-14 14:21:00.768543: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-05-14 14:21:00.769670: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-05-14 14:21:00.877662: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-05-14 14:21:00.878511: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-05-14 14:21:00.879504: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-05-14 14:21:01.351006: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-05-14 14:21:01.352370: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-05-14 14:21:01.353194: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-05-14 14:21:01.459260: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-05-14 14:21:01.460166: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-05-14 14:21:01.460932: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1287/1289 [============================>.] - ETA: 0s - loss: 7.8918e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-14 14:21:25.614051: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-05-14 14:21:25.615071: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-05-14 14:21:25.615996: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-05-14 14:21:25.718459: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-05-14 14:21:25.719238: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-05-14 14:21:25.720085: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1289/1289 [==============================] - 27s 19ms/step - loss: 7.8840e-04 - val_loss: 7.8164e-05\n",
      "Epoch 2/100\n",
      "1289/1289 [==============================] - 25s 19ms/step - loss: 9.9846e-05 - val_loss: 7.2077e-05\n",
      "Epoch 3/100\n",
      "1289/1289 [==============================] - 25s 20ms/step - loss: 8.5669e-05 - val_loss: 6.4249e-05\n",
      "Epoch 4/100\n",
      "1289/1289 [==============================] - 26s 20ms/step - loss: 6.9569e-05 - val_loss: 4.1170e-05\n",
      "Epoch 5/100\n",
      "1289/1289 [==============================] - 25s 19ms/step - loss: 5.9632e-05 - val_loss: 5.0364e-05\n",
      "Epoch 6/100\n",
      "1289/1289 [==============================] - 25s 20ms/step - loss: 5.4157e-05 - val_loss: 3.1853e-05\n",
      "Epoch 7/100\n",
      "1289/1289 [==============================] - 25s 19ms/step - loss: 5.0908e-05 - val_loss: 4.2964e-05\n",
      "Epoch 8/100\n",
      "1289/1289 [==============================] - 24s 19ms/step - loss: 4.2396e-05 - val_loss: 4.3395e-05\n",
      "Epoch 9/100\n",
      "1289/1289 [==============================] - 24s 19ms/step - loss: 4.3457e-05 - val_loss: 2.8435e-05\n",
      "Epoch 10/100\n",
      "1289/1289 [==============================] - 25s 19ms/step - loss: 4.0517e-05 - val_loss: 2.9765e-05\n",
      "Epoch 11/100\n",
      "1289/1289 [==============================] - 25s 19ms/step - loss: 4.2848e-05 - val_loss: 3.1492e-05\n",
      "Epoch 12/100\n",
      "1289/1289 [==============================] - 25s 19ms/step - loss: 4.1506e-05 - val_loss: 3.0384e-05\n",
      "Epoch 13/100\n",
      "1289/1289 [==============================] - 25s 20ms/step - loss: 3.9883e-05 - val_loss: 3.0889e-05\n",
      "Epoch 14/100\n",
      "1289/1289 [==============================] - 25s 19ms/step - loss: 4.0058e-05 - val_loss: 3.8815e-05\n",
      "Epoch 15/100\n",
      "1289/1289 [==============================] - 25s 19ms/step - loss: 4.2762e-05 - val_loss: 3.6661e-05\n",
      "Epoch 16/100\n",
      "1289/1289 [==============================] - 25s 19ms/step - loss: 3.9150e-05 - val_loss: 3.1952e-05\n",
      "Epoch 17/100\n",
      "1289/1289 [==============================] - 25s 19ms/step - loss: 3.9478e-05 - val_loss: 3.8302e-05\n",
      "Epoch 18/100\n",
      "1289/1289 [==============================] - 25s 19ms/step - loss: 3.8967e-05 - val_loss: 4.0212e-05\n",
      "Epoch 19/100\n",
      "1289/1289 [==============================] - 24s 19ms/step - loss: 3.6712e-05 - val_loss: 2.8343e-05\n",
      "Epoch 20/100\n",
      "1289/1289 [==============================] - 25s 19ms/step - loss: 3.9408e-05 - val_loss: 5.4882e-05\n",
      "Epoch 21/100\n",
      "1289/1289 [==============================] - 24s 19ms/step - loss: 3.8929e-05 - val_loss: 3.3697e-05\n",
      "Epoch 22/100\n",
      "1289/1289 [==============================] - 24s 19ms/step - loss: 3.5234e-05 - val_loss: 2.9032e-05\n",
      "Epoch 23/100\n",
      "1289/1289 [==============================] - 25s 19ms/step - loss: 3.7906e-05 - val_loss: 2.9906e-05\n",
      "Epoch 24/100\n",
      "1289/1289 [==============================] - 25s 19ms/step - loss: 3.5627e-05 - val_loss: 3.3661e-05\n",
      "Epoch 25/100\n",
      "1289/1289 [==============================] - 25s 19ms/step - loss: 3.6798e-05 - val_loss: 2.8551e-05\n",
      "Epoch 26/100\n",
      "1289/1289 [==============================] - 25s 19ms/step - loss: 3.6120e-05 - val_loss: 3.2591e-05\n",
      "Epoch 27/100\n",
      "1289/1289 [==============================] - 25s 19ms/step - loss: 3.5785e-05 - val_loss: 2.9842e-05\n",
      "Epoch 28/100\n",
      "1289/1289 [==============================] - 25s 19ms/step - loss: 3.6354e-05 - val_loss: 2.8321e-05\n",
      "Epoch 29/100\n",
      "1289/1289 [==============================] - 25s 19ms/step - loss: 3.6821e-05 - val_loss: 2.9753e-05\n",
      "Epoch 30/100\n",
      "1289/1289 [==============================] - 25s 19ms/step - loss: 3.6768e-05 - val_loss: 2.9911e-05\n",
      "Epoch 31/100\n",
      "1289/1289 [==============================] - 25s 19ms/step - loss: 3.6755e-05 - val_loss: 3.1145e-05\n",
      "Epoch 32/100\n",
      "1289/1289 [==============================] - 25s 19ms/step - loss: 3.4864e-05 - val_loss: 2.8152e-05\n",
      "Epoch 33/100\n",
      "1289/1289 [==============================] - 25s 19ms/step - loss: 3.6156e-05 - val_loss: 2.9470e-05\n",
      "Epoch 34/100\n",
      "1289/1289 [==============================] - 25s 19ms/step - loss: 3.6775e-05 - val_loss: 4.6142e-05\n",
      "Epoch 35/100\n",
      "1289/1289 [==============================] - 25s 19ms/step - loss: 3.6616e-05 - val_loss: 3.3602e-05\n",
      "Epoch 36/100\n",
      "1289/1289 [==============================] - 25s 19ms/step - loss: 3.5163e-05 - val_loss: 3.8016e-05\n",
      "Epoch 37/100\n",
      "1289/1289 [==============================] - 25s 19ms/step - loss: 3.6035e-05 - val_loss: 3.0639e-05\n",
      "Epoch 38/100\n",
      "1289/1289 [==============================] - 25s 19ms/step - loss: 3.5925e-05 - val_loss: 2.9211e-05\n",
      "Epoch 39/100\n",
      "1289/1289 [==============================] - 25s 19ms/step - loss: 3.5467e-05 - val_loss: 3.1689e-05\n",
      "Epoch 40/100\n",
      "1289/1289 [==============================] - 25s 19ms/step - loss: 3.5184e-05 - val_loss: 3.1813e-05\n",
      "Epoch 41/100\n",
      "1289/1289 [==============================] - 25s 19ms/step - loss: 3.4649e-05 - val_loss: 2.8413e-05\n",
      "Epoch 42/100\n",
      "1289/1289 [==============================] - 25s 19ms/step - loss: 3.4613e-05 - val_loss: 3.1220e-05\n",
      "Epoch 43/100\n",
      "1289/1289 [==============================] - 25s 19ms/step - loss: 3.5016e-05 - val_loss: 3.8733e-05\n",
      "Epoch 44/100\n",
      "1289/1289 [==============================] - 25s 19ms/step - loss: 3.5978e-05 - val_loss: 3.6559e-05\n",
      "Epoch 45/100\n",
      "1289/1289 [==============================] - 25s 19ms/step - loss: 3.4035e-05 - val_loss: 2.7967e-05\n",
      "Epoch 46/100\n",
      "1289/1289 [==============================] - 25s 19ms/step - loss: 3.4447e-05 - val_loss: 4.5595e-05\n",
      "Epoch 47/100\n",
      "1289/1289 [==============================] - 25s 20ms/step - loss: 3.5256e-05 - val_loss: 3.2188e-05\n",
      "Epoch 48/100\n",
      "1289/1289 [==============================] - 25s 20ms/step - loss: 3.4381e-05 - val_loss: 3.6995e-05\n",
      "Epoch 49/100\n",
      "1289/1289 [==============================] - 25s 20ms/step - loss: 3.4675e-05 - val_loss: 4.2427e-05\n",
      "Epoch 50/100\n",
      "1289/1289 [==============================] - 25s 20ms/step - loss: 3.5401e-05 - val_loss: 4.5059e-05\n",
      "Epoch 51/100\n",
      "1289/1289 [==============================] - 25s 20ms/step - loss: 3.4427e-05 - val_loss: 2.8550e-05\n",
      "Epoch 52/100\n",
      "1289/1289 [==============================] - 25s 20ms/step - loss: 3.5525e-05 - val_loss: 3.6632e-05\n",
      "Epoch 53/100\n",
      "1289/1289 [==============================] - 25s 19ms/step - loss: 3.4352e-05 - val_loss: 2.9165e-05\n",
      "Epoch 54/100\n",
      "1289/1289 [==============================] - 25s 19ms/step - loss: 3.4219e-05 - val_loss: 3.1690e-05\n",
      "Epoch 55/100\n",
      "1289/1289 [==============================] - 25s 19ms/step - loss: 3.3923e-05 - val_loss: 2.8857e-05\n",
      "Epoch 56/100\n",
      "1289/1289 [==============================] - 25s 20ms/step - loss: 3.5594e-05 - val_loss: 3.3393e-05\n",
      "Epoch 57/100\n",
      "1289/1289 [==============================] - 25s 19ms/step - loss: 3.4246e-05 - val_loss: 2.9074e-05\n",
      "Epoch 58/100\n",
      "1289/1289 [==============================] - 25s 19ms/step - loss: 3.4779e-05 - val_loss: 2.8282e-05\n",
      "Epoch 59/100\n",
      "1289/1289 [==============================] - 25s 19ms/step - loss: 3.4662e-05 - val_loss: 3.1931e-05\n",
      "Epoch 60/100\n",
      "1289/1289 [==============================] - 25s 19ms/step - loss: 3.4606e-05 - val_loss: 2.8567e-05\n",
      "Epoch 61/100\n",
      "1289/1289 [==============================] - 25s 20ms/step - loss: 3.4692e-05 - val_loss: 3.4304e-05\n",
      "Epoch 62/100\n",
      "1289/1289 [==============================] - 25s 20ms/step - loss: 3.4508e-05 - val_loss: 2.7958e-05\n",
      "Epoch 63/100\n",
      "1289/1289 [==============================] - 25s 19ms/step - loss: 3.3420e-05 - val_loss: 3.0050e-05\n",
      "Epoch 64/100\n",
      "1289/1289 [==============================] - 25s 19ms/step - loss: 3.4235e-05 - val_loss: 3.6873e-05\n",
      "Epoch 65/100\n",
      "1289/1289 [==============================] - 25s 19ms/step - loss: 3.4754e-05 - val_loss: 2.8670e-05\n",
      "Epoch 66/100\n",
      "1289/1289 [==============================] - 25s 19ms/step - loss: 3.4334e-05 - val_loss: 4.2855e-05\n",
      "Epoch 67/100\n",
      "1289/1289 [==============================] - 25s 19ms/step - loss: 3.4544e-05 - val_loss: 2.9659e-05\n",
      "Epoch 68/100\n",
      "1289/1289 [==============================] - 25s 19ms/step - loss: 3.5151e-05 - val_loss: 2.7813e-05\n",
      "Epoch 69/100\n",
      "1289/1289 [==============================] - 25s 19ms/step - loss: 3.4000e-05 - val_loss: 3.3596e-05\n",
      "Epoch 70/100\n",
      "1289/1289 [==============================] - 25s 19ms/step - loss: 3.3656e-05 - val_loss: 2.8576e-05\n",
      "Epoch 71/100\n",
      "1289/1289 [==============================] - 25s 19ms/step - loss: 3.3496e-05 - val_loss: 4.2908e-05\n",
      "Epoch 72/100\n",
      "1289/1289 [==============================] - 25s 19ms/step - loss: 3.4610e-05 - val_loss: 3.1449e-05\n",
      "Epoch 73/100\n",
      "1289/1289 [==============================] - 25s 19ms/step - loss: 3.3281e-05 - val_loss: 3.3019e-05\n",
      "Epoch 74/100\n",
      "1289/1289 [==============================] - 25s 19ms/step - loss: 3.4353e-05 - val_loss: 2.7874e-05\n",
      "Epoch 75/100\n",
      "1289/1289 [==============================] - 25s 19ms/step - loss: 3.4205e-05 - val_loss: 4.1958e-05\n",
      "Epoch 76/100\n",
      "1289/1289 [==============================] - 25s 19ms/step - loss: 3.4485e-05 - val_loss: 3.2890e-05\n",
      "Epoch 77/100\n",
      "1289/1289 [==============================] - 25s 19ms/step - loss: 3.3944e-05 - val_loss: 3.5042e-05\n",
      "Epoch 78/100\n",
      "1289/1289 [==============================] - 25s 19ms/step - loss: 3.3559e-05 - val_loss: 2.8871e-05\n",
      "Epoch 79/100\n",
      "1289/1289 [==============================] - 25s 19ms/step - loss: 3.4350e-05 - val_loss: 4.5385e-05\n",
      "Epoch 80/100\n",
      "1289/1289 [==============================] - 25s 19ms/step - loss: 3.3785e-05 - val_loss: 2.8559e-05\n",
      "Epoch 81/100\n",
      "1289/1289 [==============================] - 25s 19ms/step - loss: 3.4339e-05 - val_loss: 2.8972e-05\n",
      "Epoch 82/100\n",
      "1289/1289 [==============================] - 25s 19ms/step - loss: 3.4351e-05 - val_loss: 3.0231e-05\n",
      "Epoch 83/100\n",
      "1289/1289 [==============================] - 25s 19ms/step - loss: 3.3535e-05 - val_loss: 2.8285e-05\n",
      "Epoch 84/100\n",
      "1289/1289 [==============================] - 25s 19ms/step - loss: 3.3289e-05 - val_loss: 4.8283e-05\n",
      "Epoch 85/100\n",
      "1289/1289 [==============================] - 25s 19ms/step - loss: 3.3958e-05 - val_loss: 3.2629e-05\n",
      "Epoch 86/100\n",
      "1289/1289 [==============================] - 25s 19ms/step - loss: 3.4081e-05 - val_loss: 3.4287e-05\n",
      "Epoch 87/100\n",
      "1289/1289 [==============================] - 25s 19ms/step - loss: 3.2867e-05 - val_loss: 2.9066e-05\n",
      "Epoch 88/100\n",
      "1289/1289 [==============================] - 25s 19ms/step - loss: 3.4023e-05 - val_loss: 3.6389e-05\n",
      "Epoch 89/100\n",
      "1289/1289 [==============================] - 25s 19ms/step - loss: 3.3679e-05 - val_loss: 2.7890e-05\n",
      "Epoch 90/100\n",
      "1289/1289 [==============================] - 25s 19ms/step - loss: 3.4644e-05 - val_loss: 3.5641e-05\n",
      "Epoch 91/100\n",
      "1289/1289 [==============================] - 25s 19ms/step - loss: 3.3888e-05 - val_loss: 3.6940e-05\n",
      "Epoch 92/100\n",
      "1289/1289 [==============================] - 25s 19ms/step - loss: 3.3819e-05 - val_loss: 2.9327e-05\n",
      "Epoch 93/100\n",
      "1289/1289 [==============================] - 25s 19ms/step - loss: 3.4163e-05 - val_loss: 2.8484e-05\n",
      "Epoch 94/100\n",
      "1289/1289 [==============================] - 25s 19ms/step - loss: 3.3857e-05 - val_loss: 2.8374e-05\n",
      "Epoch 95/100\n",
      "1289/1289 [==============================] - 25s 19ms/step - loss: 3.3206e-05 - val_loss: 2.8363e-05\n",
      "Epoch 96/100\n",
      "1289/1289 [==============================] - 25s 19ms/step - loss: 3.3914e-05 - val_loss: 4.2995e-05\n",
      "Epoch 97/100\n",
      "1289/1289 [==============================] - 25s 19ms/step - loss: 3.3344e-05 - val_loss: 2.9473e-05\n",
      "Epoch 98/100\n",
      "1289/1289 [==============================] - 24s 19ms/step - loss: 3.4087e-05 - val_loss: 3.0305e-05\n",
      "Epoch 99/100\n",
      "1289/1289 [==============================] - 25s 19ms/step - loss: 3.4078e-05 - val_loss: 2.8965e-05\n",
      "Epoch 100/100\n",
      "1289/1289 [==============================] - 25s 19ms/step - loss: 3.4111e-05 - val_loss: 2.7866e-05\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Este código carga los datos históricos de precios de Bitcoin desde el cliente de Binance y los procesa \n",
    "para la entrada del modelo. Luego, divide los datos en conjuntos de entrenamiento y validación, \n",
    "define un modelo de red neuronal LSTM y lo entrena durante 100 épocas utilizando el optimizador \n",
    "Adam y la función de pérdida de error cuadrático medio (mse). Finalmente, guarda el modelo \n",
    "entrenado en un archivo llamado \"btc_price_prediction_model.h5\"\n",
    "'''\n",
    "\n",
    "'''\n",
    "TO-DO LIST:\n",
    "1. Implementar al entrenamiento en el corto-mediano plazo: \n",
    "    * la lectura de opiniones en RRSS, \n",
    "    * análisis de traders externos \n",
    "    * adición de indicadores técnicos\n",
    "2. Cómo aumentar las redes neuronales?\n",
    "3. A la hora de graficar los precios de predicción, hay que hacerlo a modo de área predecida y no de precio en especifico \n",
    "'''\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from binance.client import Client\n",
    "\n",
    "# Autenticación en la API de Binance\n",
    "api_key = 'T4Bw573BSJCbaHscSo8jn37lE1SOoGsNircF8f7B061WIKBxNkP5nx68vvAev9uk'\n",
    "api_secret = 'p7VN6HHKR6ZtGkfEeZC3FongJPkR2yr7AjPtPDC8IeaOkASdKZYCKyTuiIfGw57q'\n",
    "client = Client(api_key, api_secret)\n",
    "\n",
    "# Obtención de los datos históricos de precios de Bitcoin\n",
    "klines = client.futures_historical_klines(\"BTCUSDT\", Client.KLINE_INTERVAL_1HOUR, \"13 year ago UTC\") #intervalo de 15minutes a 10 años\n",
    "\n",
    "# Creación de un dataframe con los datos\n",
    "data = pd.DataFrame(klines, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume', 'close_time', \n",
    "                                     'quote_asset_volume', 'number_of_trades', 'taker_buy_base_asset_volume', \n",
    "                                     'taker_buy_quote_asset_volume', 'ignore'])\n",
    "\n",
    "# Eliminación de las columnas que no se van a utilizar\n",
    "data = data.drop(['timestamp', 'high', 'low', 'volume', 'close_time', 'quote_asset_volume', 'number_of_trades', \n",
    "                  'taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume', 'ignore'], axis=1)\n",
    "\n",
    "# Conversión de los precios a float\n",
    "data['open'] = data['open'].astype(float)\n",
    "data['close'] = data['close'].astype(float)\n",
    "\n",
    "# Normalizar los datos\n",
    "max_value = data['close'].max()\n",
    "min_value = data['close'].min()\n",
    "data['close'] = (data['close'] - min_value) / (max_value - min_value)\n",
    "\n",
    "# Procesar los datos para la entrada del modelo\n",
    "data['next_close'] = data['close'].shift(-1)\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "n = 48  # Número de pasos de tiempo en la secuencia\n",
    "\n",
    "for i in range(n, len(data)):\n",
    "    X.append(data['close'].values[i-n:i])\n",
    "    y.append(data['next_close'].values[i])\n",
    "\n",
    "# Convertir a numpy arrays\n",
    "X, y = np.array(X), np.array(y)\n",
    "\n",
    "# Redimensionar X a la forma adecuada para la entrada LSTM [muestras, pasos de tiempo, características]\n",
    "X = np.reshape(X, (X.shape[0], X.shape[1], 1))\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y validación\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Definir el modelo de red neuronal\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.LSTM(50, return_sequences=True, input_shape=(X.shape[1], 1)),\n",
    "    tf.keras.layers.LSTM(50),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "# Entrenar el modelo\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=20, validation_data=(X_val, y_val))\n",
    "\n",
    "# Guardar el modelo entrenado\n",
    "model.save('modelos de entrenamiento/btc_price_prediction_model_copy.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-La función train_test_split de Sklearn es utilizada para dividir el conjunto de datos en dos subconjuntos: uno para entrenamiento y otro para prueba. La función toma cuatro argumentos: los datos de entrada (data.iloc[:, :-1]), las etiquetas (data.iloc[:, -1]), el tamaño del conjunto de prueba (test_size=0.2) y la semilla aleatoria (random_state=42) para garantizar la reproducibilidad del experimento. Esta función devuelve cuatro variables: X_train, X_test, y_train y y_test. X_train y y_train son los subconjuntos de entrenamiento, mientras que X_test y y_test son los subconjuntos de prueba.\n",
    "\n",
    "-tf.keras.Sequential es una función que se utiliza para construir un modelo secuencial, es decir, una pila lineal de capas de redes neuronales. En este caso, se está construyendo un modelo con dos capas densas (Dense). La primera capa tiene 10 unidades, toma una entrada de 4 características (input_shape=[4]) y utiliza la función de activación ReLU (activation='relu'). La segunda capa tiene 3 unidades y utiliza la función de activación Softmax (activation='softmax').\n",
    "\n",
    "-model.compile se utiliza para compilar el modelo creado. Toma tres argumentos: el optimizador (optimizer=tf.keras.optimizers.Adam(0.01)), la función de pérdida (loss='sparse_categorical_crossentropy') y las métricas (metrics=['accuracy']). En este caso, se utiliza el optimizador Adam con una tasa de aprendizaje de 0.01, la función de pérdida sparse_categorical_crossentropy y la métrica de precisión (accuracy).\n",
    "\n",
    "-Después de compilar el modelo, se procede a entrenarlo utilizando la función model.fit(). Esta función recibe varios parámetros importantes:\n",
    "\n",
    "x_train y y_train: son los datos de entrenamiento que se utilizarán para ajustar el modelo.\n",
    "\n",
    "validation_data: es un conjunto de datos que se utilizará para validar el modelo durante el entrenamiento. En este caso, se utiliza el 20% de los datos de entrenamiento como conjunto de validación.\n",
    "\n",
    "epochs: es la cantidad de veces que el modelo verá todos los datos de entrenamiento durante el entrenamiento. En este caso, se utilizan 50 épocas.\n",
    "\n",
    "batch_size: es el número de muestras que se utilizarán para calcular el error y actualizar los pesos del modelo. En este caso, se utilizan 32 muestras por lote.\n",
    "\n",
    "verbose: es un valor que indica la cantidad de información que se mostrará durante el entrenamiento. En este caso, se utiliza verbose=2, lo que significa que se mostrará una barra de progreso durante el entrenamiento.\n",
    "\n",
    "callbacks: es una lista de objetos que se utilizarán durante el entrenamiento para realizar acciones específicas en ciertos momentos. En este caso, se utiliza el objeto EarlyStopping para detener el entrenamiento si el modelo deja de mejorar en el conjunto de validación.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TRADING",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
